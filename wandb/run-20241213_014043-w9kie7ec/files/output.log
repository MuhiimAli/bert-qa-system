Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2468:   0%|          | 15/6966 [00:01<09:02, 12.81it/s]












































































































































































































































































Loss: 3.9688: 100%|██████████| 6966/6966 [08:59<00:00, 12.92it/s]




Loss: 2.8713, F1: 0.4784, EM: 0.5897:  86%|████████▌ | 376/436 [00:09<00:01, 42.43it/s]
Results:
Train Loss: 3.9688
Val Loss: 2.6763
Precision: 0.4467
Recall: 0.4985
F1: 0.4711
Loss: 2.6763, F1: 0.4711, EM: 0.6208: 100%|██████████| 436/436 [00:10<00:00, 41.45it/s]













































































































































































































































































Loss: 2.1930: 100%|██████████| 6966/6966 [09:00<00:00, 12.89it/s]




Loss: 2.9842, F1: 0.4710, EM: 0.6529: 100%|██████████| 436/436 [00:10<00:00, 41.82it/s]
Training:   0%|          | 0/6966 [00:00<?, ?it/s]
Results:
Train Loss: 2.1930
Val Loss: 2.9842
Precision: 0.4575
Recall: 0.4852
F1: 0.4710












































































































































































































































































Loss: 1.4197: 100%|██████████| 6966/6966 [08:58<00:00, 12.94it/s]





Loss: 3.9092, F1: 0.5348, EM: 0.6657:  97%|█████████▋| 421/436 [00:10<00:00, 42.48it/s]
Results:
Train Loss: 1.4197
Val Loss: 3.8545
Precision: 0.5117
Recall: 0.5625

Loss: 3.8545, F1: 0.5359, EM: 0.6713: 100%|██████████| 436/436 [00:10<00:00, 41.56it/s]