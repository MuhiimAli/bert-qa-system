Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/2
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.





























































































































































Loss: 3.7677: 100%|██████████| 870/870 [05:16<00:00,  2.75it/s]


Loss: 2.9387, F1: 0.3705, EM: 0.6310:  76%|███████▋  | 42/55 [00:05<00:01,  7.95it/s]
Results:
Train Loss: 3.7677
Val Loss: 2.4092
Precision: 0.3587
Recall: 0.4619
F1: 0.4038
Loss: 2.4092, F1: 0.4038, EM: 0.6965: 100%|██████████| 55/55 [00:07<00:00,  7.84it/s]





























































































































































Loss: 1.9604: 100%|██████████| 870/870 [05:16<00:00,  2.75it/s]



Loss: 2.5397, F1: 0.4287, EM: 0.6855:  85%|████████▌ | 47/55 [00:06<00:01,  7.99it/s]
Results:
Train Loss: 1.9604
Val Loss: 2.3067
Precision: 0.3876
Recall: 0.5209

Loss: 2.3067, F1: 0.4445, EM: 0.7103: 100%|██████████| 55/55 [00:06<00:00,  7.97it/s]