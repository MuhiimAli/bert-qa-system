Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.























































































































































Loss: 4.1158: 100%|██████████| 870/870 [05:04<00:00,  2.85it/s]


Loss: 2.9269, F1: 0.3108, EM: 0.6034:  76%|███████▋  | 42/55 [00:05<00:01,  8.11it/s]
Results:
Train Loss: 4.1158
Val Loss: 2.4703
Precision: 0.2534
Recall: 0.4868
F1: 0.3333
Loss: 2.4703, F1: 0.3333, EM: 0.6644: 100%|██████████| 55/55 [00:06<00:00,  8.06it/s]
























































































































































Loss: 2.0283: 100%|██████████| 870/870 [05:06<00:00,  2.84it/s]



Loss: 2.4452, F1: 0.4155, EM: 0.7013:  91%|█████████ | 50/55 [00:06<00:00,  8.14it/s]
Results:
Train Loss: 2.0283
Val Loss: 2.2623
Precision: 0.3737
Recall: 0.5016
F1: 0.4283
Loss: 2.2623, F1: 0.4283, EM: 0.7206: 100%|██████████| 55/55 [00:06<00:00,  8.06it/s]
























































































































































Loss: 1.4483: 100%|██████████| 870/870 [05:06<00:00,  2.84it/s]


Loss: 2.7249, F1: 0.3919, EM: 0.6746:  84%|████████▎ | 46/55 [00:05<00:01,  8.23it/s]
Results:
Train Loss: 1.4483
Val Loss: 2.4019
Precision: 0.3290
Recall: 0.5202

Loss: 2.4019, F1: 0.4031, EM: 0.7091: 100%|██████████| 55/55 [00:06<00:00,  8.08it/s]