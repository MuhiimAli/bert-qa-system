Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training:   0%|          | 0/3483 [00:00<?, ?it/s]

















































































































































































































































Loss: 3.5528: 100%|██████████| 3483/3483 [08:03<00:00,  7.21it/s]




Loss: 2.6386, F1: 0.4233, EM: 0.6143:  95%|█████████▌| 208/218 [00:09<00:00, 22.55it/s]
Results:
Train Loss: 3.5528
Val Loss: 2.5897
Precision: 0.3700
Recall: 0.4967

Loss: 2.5897, F1: 0.4241, EM: 0.6219: 100%|██████████| 218/218 [00:09<00:00, 22.14it/s]