Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.































































































































































Loss: 3.6219: 100%|██████████| 1740/1740 [05:20<00:00,  5.42it/s]


Loss: 3.0044, F1: 0.3627, EM: 0.6077:  69%|██████▉   | 75/109 [00:04<00:02, 15.99it/s]
Results:
Train Loss: 3.6219
Val Loss: 2.3726
Precision: 0.2886
Recall: 0.5406
F1: 0.3763

Loss: 2.3726, F1: 0.3763, EM: 0.6902: 100%|██████████| 109/109 [00:06<00:00, 15.63it/s]
































































































































































Loss: 1.8072: 100%|██████████| 1740/1740 [05:23<00:00,  5.37it/s]



Loss: 2.4274, F1: 0.4509, EM: 0.7002:  94%|█████████▍| 103/109 [00:06<00:00, 15.93it/s]
Results:
Train Loss: 1.8072
Val Loss: 2.3298
Precision: 0.3703
Recall: 0.5900
F1: 0.4550
Loss: 2.3298, F1: 0.4550, EM: 0.7126: 100%|██████████| 109/109 [00:07<00:00, 15.57it/s]
































































































































































Loss: 1.1089: 100%|██████████| 1740/1740 [05:22<00:00,  5.39it/s]


Loss: 3.1883, F1: 0.4244, EM: 0.6667:  74%|███████▍  | 81/109 [00:05<00:01, 15.90it/s]
Results:
Train Loss: 1.1089
Val Loss: 2.5709
Precision: 0.3688
Recall: 0.5856

Loss: 2.5709, F1: 0.4526, EM: 0.7292: 100%|██████████| 109/109 [00:07<00:00, 15.55it/s]