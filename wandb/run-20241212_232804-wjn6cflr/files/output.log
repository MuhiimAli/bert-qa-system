Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 12.8280:   0%|          | 15/6966 [00:01<09:05, 12.74it/s]














































































































































































































































































Loss: 3.7458: 100%|██████████| 6966/6966 [09:02<00:00, 12.84it/s]




Loss: 2.9166, F1: 0.4521, EM: 0.6151:  92%|█████████▏| 401/436 [00:09<00:00, 41.94it/s]
Results:
Train Loss: 3.7458
Val Loss: 2.7857
Precision: 0.4170
Recall: 0.5002
F1: 0.4549
Loss: 2.7857, F1: 0.4549, EM: 0.6345: 100%|██████████| 436/436 [00:10<00:00, 41.14it/s]















































































































































































































































































Loss: 2.0024: 100%|██████████| 6966/6966 [09:03<00:00, 12.81it/s]




Loss: 3.0793, F1: 0.4945, EM: 0.6426:  92%|█████████▏| 401/436 [00:09<00:00, 42.26it/s]
Results:
Train Loss: 2.0024
Val Loss: 2.9779
Precision: 0.4602
Recall: 0.5371

Loss: 2.9779, F1: 0.4957, EM: 0.6563: 100%|██████████| 436/436 [00:10<00:00, 41.33it/s]