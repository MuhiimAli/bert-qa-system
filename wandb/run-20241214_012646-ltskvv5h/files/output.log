Training examples: 27847
Number of training batches: 3480
Evaluation examples: 1743
Number of evaluation batches: 218
Epoch 1/2
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.













































































































































































Loss: 3.3488: 100%|██████████| 3480/3480 [05:49<00:00,  9.95it/s]



Loss: 2.4507, F1: 0.3872, EM: 0.6927:  94%|█████████▍| 205/218 [00:06<00:00, 30.57it/s]
Results:
Train Loss: 3.3488
Val Loss: 2.3472
Precision: 0.3318
Recall: 0.4909
F1: 0.3960
Loss: 2.3472, F1: 0.3960, EM: 0.7063: 100%|██████████| 218/218 [00:07<00:00, 30.06it/s]














































































































































































Loss: 1.7307: 100%|██████████| 3480/3480 [05:51<00:00,  9.91it/s]



Loss: 2.6090, F1: 0.4482, EM: 0.6970:  87%|████████▋ | 189/218 [00:06<00:00, 30.85it/s]
Results:
Train Loss: 1.7307
Val Loss: 2.3870
Precision: 0.4023
Recall: 0.5539

Loss: 2.3870, F1: 0.4661, EM: 0.7235: 100%|██████████| 218/218 [00:07<00:00, 30.17it/s]