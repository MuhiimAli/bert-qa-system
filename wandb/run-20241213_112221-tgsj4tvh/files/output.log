Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.5988:   0%|          | 2/870 [00:01<08:18,  1.74it/s]

























































































































































































































Loss: 4.2790: 100%|██████████| 870/870 [07:16<00:00,  1.99it/s]



Loss: 3.0326, F1: 0.4096, EM: 0.5583:  82%|████████▏ | 45/55 [00:07<00:01,  5.87it/s]
Results:
Train Loss: 4.2790
Val Loss: 2.7318
Precision: 0.3537
Recall: 0.4619
F1: 0.4006
Loss: 2.7318, F1: 0.4006, EM: 0.6024: 100%|██████████| 55/55 [00:09<00:00,  5.79it/s]


























































































































































































































Loss: 2.1687: 100%|██████████| 870/870 [07:18<00:00,  1.99it/s]




Loss: 2.7765, F1: 0.4697, EM: 0.5892:  87%|████████▋ | 48/55 [00:08<00:01,  5.86it/s]
Results:
Train Loss: 2.1687
Val Loss: 2.5953
Precision: 0.4132
Recall: 0.5328

Loss: 2.5953, F1: 0.4655, EM: 0.6162: 100%|██████████| 55/55 [00:09<00:00,  5.77it/s]