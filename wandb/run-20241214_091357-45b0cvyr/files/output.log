Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.























































































































































Loss: 3.9855: 100%|██████████| 870/870 [05:04<00:00,  2.85it/s]


Loss: 3.0170, F1: 0.3498, EM: 0.6347:  76%|███████▋  | 42/55 [00:05<00:01,  8.13it/s]
Results:
Train Loss: 3.9855
Val Loss: 2.4700
Precision: 0.3383
Recall: 0.4490
F1: 0.3859
Loss: 2.4700, F1: 0.3859, EM: 0.6994: 100%|██████████| 55/55 [00:06<00:00,  8.04it/s]
























































































































































Loss: 2.0081: 100%|██████████| 870/870 [05:06<00:00,  2.84it/s]



Loss: 2.4563, F1: 0.4358, EM: 0.6868:  87%|████████▋ | 48/55 [00:06<00:00,  8.18it/s]
Results:
Train Loss: 2.0081
Val Loss: 2.2285
Precision: 0.4074
Recall: 0.5117
F1: 0.4537
Loss: 2.2285, F1: 0.4537, EM: 0.7137: 100%|██████████| 55/55 [00:06<00:00,  8.06it/s]
























































































































































Loss: 1.4337: 100%|██████████| 870/870 [05:06<00:00,  2.84it/s]



Loss: 2.3309, F1: 0.4217, EM: 0.7177: 100%|██████████| 55/55 [00:06<00:00,  8.05it/s]
Results:
Train Loss: 1.4337
Val Loss: 2.3309
Precision: 0.3372
Recall: 0.5628
F1: 0.4217