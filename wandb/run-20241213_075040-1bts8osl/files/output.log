Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2700:   0%|          | 9/3483 [00:01<08:00,  7.24it/s]

















































































































































































































































Loss: 3.8929: 100%|██████████| 3483/3483 [08:05<00:00,  7.18it/s]




Loss: 2.8148, F1: 0.4247, EM: 0.5896:  90%|████████▉ | 196/218 [00:09<00:00, 22.29it/s]
Results:
Train Loss: 3.8929
Val Loss: 2.7039
Precision: 0.3433
Recall: 0.5428
F1: 0.4206
Loss: 2.7039, F1: 0.4206, EM: 0.6081: 100%|██████████| 218/218 [00:09<00:00, 21.92it/s]


















































































































































































































































Loss: 1.9999: 100%|██████████| 3483/3483 [08:06<00:00,  7.16it/s]




Loss: 2.6205, F1: 0.4874, EM: 0.6347:  94%|█████████▍| 205/218 [00:09<00:00, 22.36it/s]
Results:
Train Loss: 1.9999
Val Loss: 2.5632
Precision: 0.4353
Recall: 0.5563
F1: 0.4884
Loss: 2.5632, F1: 0.4884, EM: 0.6460: 100%|██████████| 218/218 [00:09<00:00, 21.86it/s]


















































































































































































































































Loss: 1.1524: 100%|██████████| 3483/3483 [08:07<00:00,  7.14it/s]





Loss: 3.1205, F1: 0.4852, EM: 0.6535: 100%|██████████| 218/218 [00:09<00:00, 21.91it/s]
Results:
Train Loss: 1.1524
Val Loss: 3.1205
Precision: 0.4455
Recall: 0.5326
F1: 0.4852