Training examples: 27847
Number of training batches: 3480
Evaluation examples: 1743
Number of evaluation batches: 218
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.











































































































































































Loss: 3.4597: 100%|██████████| 3480/3480 [05:44<00:00, 10.11it/s]


Loss: 2.6685, F1: 0.3963, EM: 0.6715:  81%|████████  | 177/218 [00:05<00:01, 31.04it/s]
Results:
Train Loss: 3.4597
Val Loss: 2.3652
Precision: 0.3449
Recall: 0.5088
F1: 0.4111
Loss: 2.3652, F1: 0.4111, EM: 0.7114: 100%|██████████| 218/218 [00:07<00:00, 30.31it/s]












































































































































































Loss: 1.7950: 100%|██████████| 3480/3480 [05:45<00:00, 10.08it/s]


Loss: 2.3904, F1: 0.5184, EM: 0.7441: 100%|██████████| 218/218 [00:07<00:00, 30.42it/s]
Loss: 1.0019:   0%|          | 5/3480 [00:00<06:40,  8.67it/s]
Results:
Train Loss: 1.7950
Val Loss: 2.3904
Precision: 0.4806
Recall: 0.5626
F1: 0.5184











































































































































































Loss: 1.0844: 100%|██████████| 3480/3480 [05:44<00:00, 10.09it/s]



Loss: 3.4432, F1: 0.4909, EM: 0.7023:  85%|████████▍ | 185/218 [00:06<00:01, 30.89it/s]
Results:
Train Loss: 1.0844
Val Loss: 3.1021
Precision: 0.4576
Recall: 0.5713

Loss: 3.1021, F1: 0.5082, EM: 0.7326: 100%|██████████| 218/218 [00:07<00:00, 30.35it/s]