Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.





























































































































































Loss: 3.5679: 100%|██████████| 1740/1740 [05:17<00:00,  5.47it/s]



Loss: 2.8281, F1: 0.3689, EM: 0.6784:  87%|████████▋ | 95/109 [00:06<00:00, 16.03it/s]
Results:
Train Loss: 3.5679
Val Loss: 2.5303
Precision: 0.3714
Recall: 0.4238
F1: 0.3959
Loss: 2.5303, F1: 0.3959, EM: 0.7137: 100%|██████████| 109/109 [00:06<00:00, 15.68it/s]






























































































































































Loss: 1.7917: 100%|██████████| 1740/1740 [05:19<00:00,  5.45it/s]



Loss: 2.2319, F1: 0.4536, EM: 0.7220:  98%|█████████▊| 107/109 [00:06<00:00, 16.11it/s]
Results:
Train Loss: 1.7917
Val Loss: 2.1937
Precision: 0.3962
Recall: 0.5401
F1: 0.4571
Loss: 2.1937, F1: 0.4571, EM: 0.7269: 100%|██████████| 109/109 [00:06<00:00, 15.70it/s]































































































































































Loss: 1.0872: 100%|██████████| 1740/1740 [05:21<00:00,  5.41it/s]



Loss: 2.7148, F1: 0.4383, EM: 0.7075:  94%|█████████▍| 103/109 [00:06<00:00, 15.84it/s]
Results:
Train Loss: 1.0872
Val Loss: 2.5899
Precision: 0.3554
Recall: 0.5650

Loss: 2.5899, F1: 0.4363, EM: 0.7194: 100%|██████████| 109/109 [00:06<00:00, 15.64it/s]