Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
























































































































































Loss: 4.0504: 100%|██████████| 870/870 [05:07<00:00,  2.83it/s]



Loss: 2.5158, F1: 0.3587, EM: 0.6893:  96%|█████████▋| 53/55 [00:06<00:00,  8.13it/s]
Results:
Train Loss: 4.0504
Val Loss: 2.4406
Precision: 0.3271
Recall: 0.4105
F1: 0.3641
Loss: 2.4406, F1: 0.3641, EM: 0.6965: 100%|██████████| 55/55 [00:06<00:00,  8.04it/s]

























































































































































Loss: 2.0294: 100%|██████████| 870/870 [05:09<00:00,  2.82it/s]


Loss: 2.3015, F1: 0.4340, EM: 0.7172: 100%|██████████| 55/55 [00:06<00:00,  8.06it/s]
Training:   0%|          | 0/870 [00:00<?, ?it/s]
Results:
Train Loss: 2.0294
Val Loss: 2.3015
Precision: 0.4156
Recall: 0.4541
F1: 0.4340


























































































































































Loss: 1.4316: 100%|██████████| 870/870 [05:09<00:00,  2.81it/s]



Loss: 2.3768, F1: 0.4311, EM: 0.7114: 100%|██████████| 55/55 [00:06<00:00,  8.05it/s]
Results:
Train Loss: 1.4316
Val Loss: 2.3768
Precision: 0.3548
Recall: 0.5493
F1: 0.4311