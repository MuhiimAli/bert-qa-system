Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.3044:   0%|          | 9/3483 [00:01<08:01,  7.21it/s]

















































































































































































































































Loss: 3.8392: 100%|██████████| 3483/3483 [08:05<00:00,  7.18it/s]




Loss: 2.7454, F1: 0.4641, EM: 0.5928:  89%|████████▊ | 193/218 [00:08<00:01, 22.40it/s]
Results:
Train Loss: 3.8392
Val Loss: 2.5828
Precision: 0.4074
Recall: 0.4982
F1: 0.4483
Loss: 2.5828, F1: 0.4483, EM: 0.6156: 100%|██████████| 218/218 [00:09<00:00, 21.88it/s]


















































































































































































































































Loss: 1.9058: 100%|██████████| 3483/3483 [08:06<00:00,  7.16it/s]




Loss: 2.7189, F1: 0.4952, EM: 0.6318:  93%|█████████▎| 202/218 [00:09<00:00, 22.40it/s]
Results:
Train Loss: 1.9058
Val Loss: 2.6326
Precision: 0.4645
Recall: 0.5228

Loss: 2.6326, F1: 0.4919, EM: 0.6454: 100%|██████████| 218/218 [00:09<00:00, 21.89it/s]