Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/2
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


























































































































































Loss: 3.7013: 100%|██████████| 870/870 [05:11<00:00,  2.79it/s]



Loss: 2.5023, F1: 0.3596, EM: 0.6716:  96%|█████████▋| 53/55 [00:06<00:00,  8.05it/s]
Results:
Train Loss: 3.7013
Val Loss: 2.4389
Precision: 0.2867
Recall: 0.4870
F1: 0.3609
Loss: 2.4389, F1: 0.3609, EM: 0.6781: 100%|██████████| 55/55 [00:06<00:00,  7.98it/s]



























































































































































Loss: 1.9571: 100%|██████████| 870/870 [05:13<00:00,  2.78it/s]


Loss: 2.9937, F1: 0.3971, EM: 0.6217:  69%|██████▉   | 38/55 [00:04<00:02,  8.10it/s]
Results:
Train Loss: 1.9571
Val Loss: 2.2896
Precision: 0.3705
Recall: 0.5113

Loss: 2.2896, F1: 0.4296, EM: 0.7074: 100%|██████████| 55/55 [00:06<00:00,  7.98it/s]