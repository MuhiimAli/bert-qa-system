Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



























































































































































Loss: 3.9028: 100%|██████████| 870/870 [05:13<00:00,  2.78it/s]



Loss: 2.4726, F1: 0.3647, EM: 0.6875:  98%|█████████▊| 54/55 [00:06<00:00,  8.00it/s]
Results:
Train Loss: 3.9028
Val Loss: 2.4363
Precision: 0.2929
Recall: 0.4870
F1: 0.3658
Loss: 2.4363, F1: 0.3658, EM: 0.6896: 100%|██████████| 55/55 [00:06<00:00,  7.90it/s]



























































































































































Loss: 1.9735: 100%|██████████| 870/870 [05:13<00:00,  2.77it/s]



Loss: 2.3754, F1: 0.4861, EM: 0.7150:  91%|█████████ | 50/55 [00:06<00:00,  7.98it/s]
Results:
Train Loss: 1.9735
Val Loss: 2.2071
Precision: 0.4777
Recall: 0.5023
F1: 0.4897
Loss: 2.2071, F1: 0.4897, EM: 0.7315: 100%|██████████| 55/55 [00:06<00:00,  7.91it/s]





























































































































































Loss: 1.3806: 100%|██████████| 870/870 [05:16<00:00,  2.75it/s]


Loss: 2.7133, F1: 0.4327, EM: 0.6719:  80%|████████  | 44/55 [00:05<00:01,  8.09it/s]
Results:
Train Loss: 1.3806
Val Loss: 2.3485
Precision: 0.3711
Recall: 0.5725

Loss: 2.3485, F1: 0.4503, EM: 0.7120: 100%|██████████| 55/55 [00:06<00:00,  7.97it/s]