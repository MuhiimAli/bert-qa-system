Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



























































































































































Loss: 4.0458: 100%|██████████| 870/870 [05:12<00:00,  2.78it/s]


Loss: 2.9751, F1: 0.3315, EM: 0.6141:  78%|███████▊  | 43/55 [00:05<00:01,  8.00it/s]
Results:
Train Loss: 4.0458
Val Loss: 2.5297
Precision: 0.2823
Recall: 0.4594
F1: 0.3497
Loss: 2.5297, F1: 0.3497, EM: 0.6701: 100%|██████████| 55/55 [00:06<00:00,  7.96it/s]




























































































































































Loss: 2.0225: 100%|██████████| 870/870 [05:13<00:00,  2.78it/s]


Loss: 2.6337, F1: 0.4059, EM: 0.6606:  78%|███████▊  | 43/55 [00:05<00:01,  8.10it/s]
Results:
Train Loss: 2.0225
Val Loss: 2.2269
Precision: 0.3677
Recall: 0.5183
F1: 0.4302
Loss: 2.2269, F1: 0.4302, EM: 0.7114: 100%|██████████| 55/55 [00:06<00:00,  7.99it/s]



























































































































































Loss: 1.4465: 100%|██████████| 870/870 [05:13<00:00,  2.78it/s]



Loss: 2.6088, F1: 0.3935, EM: 0.6855:  87%|████████▋ | 48/55 [00:06<00:00,  8.09it/s]
Results:
Train Loss: 1.4465
Val Loss: 2.3754
Precision: 0.3206
Recall: 0.5536

Loss: 2.3754, F1: 0.4060, EM: 0.7108: 100%|██████████| 55/55 [00:06<00:00,  7.98it/s]