Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.





























































































































































Loss: 4.0786: 100%|██████████| 870/870 [05:16<00:00,  2.75it/s]


Loss: 2.9778, F1: 0.3381, EM: 0.6493:  82%|████████▏ | 45/55 [00:05<00:01,  7.90it/s]
Results:
Train Loss: 4.0786
Val Loss: 2.5220
Precision: 0.3573
Recall: 0.4013
F1: 0.3780
Loss: 2.5220, F1: 0.3780, EM: 0.7022: 100%|██████████| 55/55 [00:07<00:00,  7.84it/s]































































































































































Loss: 2.0673: 100%|██████████| 870/870 [05:19<00:00,  2.73it/s]


Loss: 2.6520, F1: 0.3859, EM: 0.6697:  78%|███████▊  | 43/55 [00:05<00:01,  7.86it/s]
Results:
Train Loss: 2.0673
Val Loss: 2.2516
Precision: 0.3595
Recall: 0.4943
F1: 0.4163
Loss: 2.2516, F1: 0.4163, EM: 0.7172: 100%|██████████| 55/55 [00:07<00:00,  7.83it/s]






























































































































































Loss: 1.4896: 100%|██████████| 870/870 [05:18<00:00,  2.73it/s]



Loss: 2.5356, F1: 0.4209, EM: 0.7000:  91%|█████████ | 50/55 [00:06<00:00,  7.96it/s]
Results:
Train Loss: 1.4896
Val Loss: 2.3719
Precision: 0.3508
Recall: 0.5396

Loss: 2.3719, F1: 0.4252, EM: 0.7143: 100%|██████████| 55/55 [00:06<00:00,  7.88it/s]