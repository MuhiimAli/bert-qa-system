Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.1856:   0%|          | 8/3483 [00:01<08:10,  7.09it/s]



















































































































































































































































Loss: 3.8562: 100%|██████████| 3483/3483 [08:08<00:00,  7.13it/s]




Loss: 2.6894, F1: 0.4302, EM: 0.6157:  98%|█████████▊| 214/218 [00:09<00:00, 22.30it/s]
Results:
Train Loss: 3.8562
Val Loss: 2.6505
Precision: 0.4042
Recall: 0.4638
F1: 0.4319
Loss: 2.6505, F1: 0.4319, EM: 0.6208: 100%|██████████| 218/218 [00:09<00:00, 21.87it/s]



















































































































































































































































Loss: 1.9795: 100%|██████████| 3483/3483 [08:09<00:00,  7.12it/s]




Loss: 2.6743, F1: 0.4456, EM: 0.6225:  91%|█████████▏| 199/218 [00:09<00:00, 22.25it/s]
Results:
Train Loss: 1.9795
Val Loss: 2.5580
Precision: 0.3991
Recall: 0.5132

Loss: 2.5580, F1: 0.4490, EM: 0.6414: 100%|██████████| 218/218 [00:09<00:00, 21.85it/s]