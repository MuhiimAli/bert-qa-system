Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
























































































































































Loss: 4.0675: 100%|██████████| 870/870 [05:06<00:00,  2.84it/s]


Loss: 2.8778, F1: 0.3283, EM: 0.6257:  82%|████████▏ | 45/55 [00:05<00:01,  8.17it/s]
Results:
Train Loss: 4.0675
Val Loss: 2.5726
Precision: 0.2613
Recall: 0.4936
F1: 0.3417
Loss: 2.5726, F1: 0.3417, EM: 0.6644: 100%|██████████| 55/55 [00:06<00:00,  8.04it/s]


























































































































































Loss: 2.0205: 100%|██████████| 870/870 [05:09<00:00,  2.81it/s]


Loss: 2.7861, F1: 0.4221, EM: 0.6570:  76%|███████▋  | 42/55 [00:05<00:01,  8.12it/s]
Results:
Train Loss: 2.0205
Val Loss: 2.2656
Precision: 0.4093
Recall: 0.5042
F1: 0.4518
Loss: 2.2656, F1: 0.4518, EM: 0.7200: 100%|██████████| 55/55 [00:06<00:00,  8.05it/s]


























































































































































Loss: 1.4208: 100%|██████████| 870/870 [05:09<00:00,  2.81it/s]


Loss: 2.7187, F1: 0.4205, EM: 0.6757:  82%|████████▏ | 45/55 [00:05<00:01,  8.15it/s]
Results:
Train Loss: 1.4208
Val Loss: 2.3686
Precision: 0.3575
Recall: 0.5357

Loss: 2.3686, F1: 0.4288, EM: 0.7126: 100%|██████████| 55/55 [00:06<00:00,  8.04it/s]