Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.






























































































































































Loss: 3.7061: 100%|██████████| 1740/1740 [05:18<00:00,  5.46it/s]


Loss: 2.3569, F1: 0.4450, EM: 0.7200: 100%|██████████| 109/109 [00:06<00:00, 15.70it/s]
Loss: 3.0500:   0%|          | 3/1740 [00:00<06:00,  4.82it/s]
Results:
Train Loss: 3.7061
Val Loss: 2.3569
Precision: 0.4541
Recall: 0.4362
F1: 0.4450
































































































































































Loss: 1.8918: 100%|██████████| 1740/1740 [05:21<00:00,  5.42it/s]


Loss: 2.5823, F1: 0.4324, EM: 0.6813:  82%|████████▏ | 89/109 [00:05<00:01, 16.07it/s]
Results:
Train Loss: 1.8918
Val Loss: 2.2592
Precision: 0.3572
Recall: 0.5602
F1: 0.4363
Loss: 2.2592, F1: 0.4363, EM: 0.7183: 100%|██████████| 109/109 [00:06<00:00, 15.69it/s]































































































































































Loss: 1.2174: 100%|██████████| 1740/1740 [05:21<00:00,  5.42it/s]



Loss: 2.7455, F1: 0.4659, EM: 0.6940:  87%|████████▋ | 95/109 [00:06<00:00, 16.01it/s]
Results:
Train Loss: 1.2174
Val Loss: 2.5224
Precision: 0.3936
Recall: 0.5800

Loss: 2.5224, F1: 0.4690, EM: 0.7154: 100%|██████████| 109/109 [00:06<00:00, 15.66it/s]