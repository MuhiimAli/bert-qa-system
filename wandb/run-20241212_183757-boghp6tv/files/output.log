Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2264:   0%|          | 4/1741 [00:01<08:16,  3.50it/s]



































































































































































































































Loss: 4.0525: 100%|██████████| 1741/1741 [07:37<00:00,  3.80it/s]




Loss: 2.8877, F1: 0.4037, EM: 0.5645:  85%|████████▌ | 93/109 [00:08<00:01, 11.40it/s]
Results:
Train Loss: 4.0525
Val Loss: 2.6770
Precision: 0.3542
Recall: 0.4785
F1: 0.4071
Loss: 2.6770, F1: 0.4071, EM: 0.6024: 100%|██████████| 109/109 [00:09<00:00, 11.16it/s]


































































































































































































































Loss: 2.0770: 100%|██████████| 1741/1741 [07:39<00:00,  3.79it/s]




Loss: 2.7847, F1: 0.4501, EM: 0.6014:  82%|████████▏ | 89/109 [00:08<00:01, 11.42it/s]
Results:
Train Loss: 2.0770
Val Loss: 2.5191
Precision: 0.3938
Recall: 0.5126

Loss: 2.5191, F1: 0.4454, EM: 0.6408: 100%|██████████| 109/109 [00:09<00:00, 11.16it/s]