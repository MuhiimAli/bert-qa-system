Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.































































































































































Loss: 3.5910: 100%|██████████| 1740/1740 [05:21<00:00,  5.40it/s]



Loss: 2.6414, F1: 0.4226, EM: 0.6755:  85%|████████▌ | 93/109 [00:06<00:01, 15.74it/s]
Results:
Train Loss: 3.5910
Val Loss: 2.3588
Precision: 0.4368
Recall: 0.4599
F1: 0.4480
Loss: 2.3588, F1: 0.4480, EM: 0.7126: 100%|██████████| 109/109 [00:07<00:00, 15.47it/s]


































































































































































Loss: 1.8146: 100%|██████████| 1740/1740 [05:26<00:00,  5.32it/s]



Loss: 2.3354, F1: 0.4490, EM: 0.7131:  96%|█████████▋| 105/109 [00:06<00:00, 15.88it/s]
Results:
Train Loss: 1.8146
Val Loss: 2.2622
Precision: 0.3911
Recall: 0.5304
F1: 0.4502
Loss: 2.2622, F1: 0.4502, EM: 0.7217: 100%|██████████| 109/109 [00:06<00:00, 15.60it/s]


































































































































































Loss: 1.0981: 100%|██████████| 1740/1740 [05:27<00:00,  5.32it/s]



Loss: 2.6489, F1: 0.4529, EM: 0.7161:  96%|█████████▋| 105/109 [00:06<00:00, 15.83it/s]
Results:
Train Loss: 1.0981
Val Loss: 2.5681
Precision: 0.3841
Recall: 0.5701

Loss: 2.5681, F1: 0.4590, EM: 0.7252: 100%|██████████| 109/109 [00:07<00:00, 15.54it/s]