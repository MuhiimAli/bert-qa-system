Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.1303:   0%|          | 15/6966 [00:01<09:06, 12.72it/s]













































































































































































































































































Loss: 3.9348: 100%|██████████| 6966/6966 [09:01<00:00, 12.86it/s]





Loss: 2.8289, F1: 0.3996, EM: 0.6127: 100%|██████████| 436/436 [00:10<00:00, 41.39it/s]
Results:
Train Loss: 3.9348
Val Loss: 2.8289
Precision: 0.3436
Recall: 0.4774
F1: 0.3996
Epoch 2/2














































































































































































































































































Loss: 2.1752: 100%|██████████| 6966/6966 [09:03<00:00, 12.82it/s]




Loss: 3.0083, F1: 0.4738, EM: 0.6233:  86%|████████▌ | 376/436 [00:09<00:01, 42.28it/s]
Results:
Train Loss: 2.1752
Val Loss: 2.8344
Precision: 0.4208
Recall: 0.5309

Loss: 2.8344, F1: 0.4695, EM: 0.6495: 100%|██████████| 436/436 [00:10<00:00, 41.38it/s]