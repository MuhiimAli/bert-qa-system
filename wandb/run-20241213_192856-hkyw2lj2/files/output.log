Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


































































































































































Loss: 3.6326: 100%|██████████| 1740/1740 [05:26<00:00,  5.34it/s]


Loss: 2.8002, F1: 0.3732, EM: 0.6580:  82%|████████▏ | 89/109 [00:05<00:01, 15.89it/s]
Results:
Train Loss: 3.6326
Val Loss: 2.3958
Precision: 0.3643
Recall: 0.4531
F1: 0.4039
Loss: 2.3958, F1: 0.4039, EM: 0.7068: 100%|██████████| 109/109 [00:06<00:00, 15.60it/s]



































































































































































Loss: 1.8065: 100%|██████████| 1740/1740 [05:28<00:00,  5.30it/s]


Loss: 2.3234, F1: 0.4755, EM: 0.7292: 100%|██████████| 109/109 [00:07<00:00, 15.56it/s]
Loss: 1.2173:   0%|          | 1/1740 [00:00<08:24,  3.45it/s]
Results:
Train Loss: 1.8065
Val Loss: 2.3234
Precision: 0.4458
Recall: 0.5093
F1: 0.4755




































































































































































Loss: 1.1345: 100%|██████████| 1740/1740 [05:29<00:00,  5.28it/s]



Loss: 2.6030, F1: 0.4977, EM: 0.7220:  98%|█████████▊| 107/109 [00:06<00:00, 15.74it/s]
Results:
Train Loss: 1.1345
Val Loss: 2.5567
Precision: 0.4396
Recall: 0.5691

Loss: 2.5567, F1: 0.4960, EM: 0.7263: 100%|██████████| 109/109 [00:07<00:00, 15.42it/s]