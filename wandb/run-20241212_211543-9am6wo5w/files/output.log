Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.0792:   0%|          | 15/6966 [00:01<09:08, 12.68it/s]













































































































































































































































































Loss: 3.7907: 100%|██████████| 6966/6966 [09:01<00:00, 12.87it/s]




Loss: 3.0874, F1: 0.4208, EM: 0.5909:  87%|████████▋ | 381/436 [00:09<00:01, 42.47it/s]
Results:
Train Loss: 3.7907
Val Loss: 2.8479
Precision: 0.3595
Recall: 0.4901
F1: 0.4148
Loss: 2.8479, F1: 0.4148, EM: 0.6219: 100%|██████████| 436/436 [00:10<00:00, 41.65it/s]













































































































































































































































































Loss: 2.0314: 100%|██████████| 6966/6966 [09:01<00:00, 12.87it/s]





Loss: 3.0942, F1: 0.4909, EM: 0.6467:  98%|█████████▊| 426/436 [00:10<00:00, 42.44it/s]
Results:
Train Loss: 2.0314
Val Loss: 3.0486
Precision: 0.4549
Recall: 0.5352

Loss: 3.0486, F1: 0.4918, EM: 0.6512: 100%|██████████| 436/436 [00:10<00:00, 41.59it/s]