Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


































































































































































Loss: 3.6216: 100%|██████████| 1740/1740 [05:27<00:00,  5.31it/s]



Loss: 2.6218, F1: 0.3896, EM: 0.6875:  89%|████████▉ | 97/109 [00:06<00:00, 15.64it/s]
Results:
Train Loss: 3.6216
Val Loss: 2.4059
Precision: 0.3502
Recall: 0.4832
F1: 0.4061
Loss: 2.4059, F1: 0.4061, EM: 0.7131: 100%|██████████| 109/109 [00:07<00:00, 15.39it/s]


































































































































































Loss: 1.8233: 100%|██████████| 1740/1740 [05:27<00:00,  5.32it/s]



Loss: 2.4281, F1: 0.4897, EM: 0.7157:  93%|█████████▎| 101/109 [00:06<00:00, 15.65it/s]
Results:
Train Loss: 1.8233
Val Loss: 2.2914
Precision: 0.4829
Recall: 0.5197
F1: 0.5006
Loss: 2.2914, F1: 0.5006, EM: 0.7315: 100%|██████████| 109/109 [00:07<00:00, 15.23it/s]

































































































































































Loss: 1.1268: 100%|██████████| 1740/1740 [05:25<00:00,  5.34it/s]



Loss: 2.9109, F1: 0.4783, EM: 0.7015:  85%|████████▌ | 93/109 [00:06<00:01, 15.27it/s]
Results:
Train Loss: 1.1268
Val Loss: 2.6076
Precision: 0.4311
Recall: 0.5762

Loss: 2.6076, F1: 0.4932, EM: 0.7298: 100%|██████████| 109/109 [00:07<00:00, 15.34it/s]