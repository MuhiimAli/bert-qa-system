Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 12.9887:   0%|          | 15/6966 [00:01<09:02, 12.81it/s]













































































































































































































































































Loss: 3.8546: 100%|██████████| 6966/6966 [09:01<00:00, 12.87it/s]




Loss: 3.0531, F1: 0.4392, EM: 0.5885:  84%|████████▍ | 366/436 [00:08<00:01, 42.33it/s]
Results:
Train Loss: 3.8546
Val Loss: 2.7609
Precision: 0.4100
Recall: 0.4668
F1: 0.4366
Loss: 2.7609, F1: 0.4366, EM: 0.6299: 100%|██████████| 436/436 [00:10<00:00, 41.49it/s]














































































































































































































































































Loss: 2.0834: 100%|██████████| 6966/6966 [09:02<00:00, 12.84it/s]





Loss: 2.8910, F1: 0.4899, EM: 0.6441:  99%|█████████▉| 431/436 [00:10<00:00, 42.27it/s]
Results:
Train Loss: 2.0834
Val Loss: 2.8841
Precision: 0.4348
Recall: 0.5616

Loss: 2.8841, F1: 0.4902, EM: 0.6454: 100%|██████████| 436/436 [00:10<00:00, 41.41it/s]