Training examples: 27847
Number of training batches: 435
Evaluation examples: 1743
Number of evaluation batches: 28
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/2
















































































































































































































Loss: 4.2127: 100%|██████████| 435/435 [06:59<00:00,  1.04it/s]




Loss: 2.8667, F1: 0.3825, EM: 0.6549:  86%|████████▌ | 24/28 [00:08<00:01,  3.00it/s]
Results:
Train Loss: 4.2127
Val Loss: 2.5760
Precision: 0.3728
Recall: 0.4381
F1: 0.4029
Loss: 2.5760, F1: 0.4029, EM: 0.6879: 100%|██████████| 28/28 [00:09<00:00,  3.01it/s]

















































































































































































































Loss: 2.2308: 100%|██████████| 435/435 [07:00<00:00,  1.03it/s]




Loss: 2.5798, F1: 0.3817, EM: 0.6769:  89%|████████▉ | 25/28 [00:08<00:00,  3.02it/s]
Results:
Train Loss: 2.2308
Val Loss: 2.3677
Precision: 0.3358
Recall: 0.4606

Loss: 2.3677, F1: 0.3884, EM: 0.6948: 100%|██████████| 28/28 [00:09<00:00,  3.03it/s]