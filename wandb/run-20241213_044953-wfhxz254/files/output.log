Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.1253:   0%|          | 15/6966 [00:01<08:53, 13.02it/s]









































































































































































































































































Loss: 3.5468: 100%|██████████| 6966/6966 [08:52<00:00, 13.07it/s]




Loss: 2.7917, F1: 0.4343, EM: 0.6088:  92%|█████████▏| 401/436 [00:09<00:00, 43.16it/s]
Results:
Train Loss: 3.5468
Val Loss: 2.6888
Precision: 0.3865
Recall: 0.4935

Loss: 2.6888, F1: 0.4335, EM: 0.6271: 100%|██████████| 436/436 [00:10<00:00, 42.31it/s]