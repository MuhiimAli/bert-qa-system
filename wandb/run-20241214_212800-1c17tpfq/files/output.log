Training examples: 27866
Number of training batches: 3483
Evaluation examples: 1743
Number of evaluation batches: 218
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.







































































































































































Loss: 4.1018: 100%|██████████| 3483/3483 [05:37<00:00, 10.33it/s]



Loss: 2.7007, F1: 0.3912, EM: 0.6377:  98%|█████████▊| 213/218 [00:06<00:00, 32.05it/s]
Results:
Train Loss: 4.1018
Val Loss: 2.6886
Precision: 0.4027
Recall: 0.3789
F1: 0.3904
Loss: 2.6886, F1: 0.3904, EM: 0.6397: 100%|██████████| 218/218 [00:07<00:00, 30.80it/s]







































































































































































Loss: 2.1048: 100%|██████████| 3483/3483 [05:37<00:00, 10.32it/s]



Loss: 2.5961, F1: 0.3982, EM: 0.6423:  96%|█████████▌| 209/218 [00:06<00:00, 32.17it/s]
Results:
Train Loss: 2.1048
Val Loss: 2.5421
Precision: 0.3519
Recall: 0.4484
F1: 0.3944
Loss: 2.5421, F1: 0.3944, EM: 0.6506: 100%|██████████| 218/218 [00:07<00:00, 30.86it/s]








































































































































































Loss: 1.3026: 100%|██████████| 3483/3483 [05:38<00:00, 10.30it/s]


Loss: 3.3470, F1: 0.4525, EM: 0.6243:  83%|████████▎ | 181/218 [00:05<00:01, 32.17it/s]
Results:
Train Loss: 1.3026
Val Loss: 3.1354
Precision: 0.3849
Recall: 0.4830

Loss: 3.1354, F1: 0.4284, EM: 0.6563: 100%|██████████| 218/218 [00:07<00:00, 30.99it/s]