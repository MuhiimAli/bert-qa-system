Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



























































































































































Loss: 3.9255: 100%|██████████| 870/870 [05:12<00:00,  2.79it/s]


Loss: 2.6904, F1: 0.3407, EM: 0.6556:  85%|████████▌ | 47/55 [00:05<00:00,  8.05it/s]
Results:
Train Loss: 3.9255
Val Loss: 2.4010
Precision: 0.2883
Recall: 0.4791
F1: 0.3600
Loss: 2.4010, F1: 0.3600, EM: 0.6908: 100%|██████████| 55/55 [00:06<00:00,  7.98it/s]




























































































































































Loss: 1.9537: 100%|██████████| 870/870 [05:13<00:00,  2.77it/s]


Loss: 2.7036, F1: 0.4000, EM: 0.6672:  78%|███████▊  | 43/55 [00:05<00:01,  8.06it/s]
Results:
Train Loss: 1.9537
Val Loss: 2.2477
Precision: 0.3718
Recall: 0.5171
F1: 0.4326
Loss: 2.2477, F1: 0.4326, EM: 0.7212: 100%|██████████| 55/55 [00:06<00:00,  7.99it/s]




























































































































































Loss: 1.3258: 100%|██████████| 870/870 [05:14<00:00,  2.77it/s]



Loss: 2.4329, F1: 0.4551, EM: 0.7140:  96%|█████████▋| 53/55 [00:06<00:00,  8.04it/s]
Results:
Train Loss: 1.3258
Val Loss: 2.3577
Precision: 0.3951
Recall: 0.5481

Loss: 2.3577, F1: 0.4592, EM: 0.7200: 100%|██████████| 55/55 [00:06<00:00,  7.91it/s]