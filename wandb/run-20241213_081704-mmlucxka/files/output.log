Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 12.9898:   0%|          | 9/3483 [00:01<07:59,  7.25it/s]

















































































































































































































































Loss: 3.6397: 100%|██████████| 3483/3483 [08:04<00:00,  7.19it/s]




Loss: 2.6497, F1: 0.3793, EM: 0.6020: 100%|█████████▉| 217/218 [00:09<00:00, 22.46it/s]
Results:
Train Loss: 3.6397
Val Loss: 2.6447
Precision: 0.3167
Recall: 0.4737

Loss: 2.6447, F1: 0.3796, EM: 0.6030: 100%|██████████| 218/218 [00:09<00:00, 21.91it/s]