Training examples: 27866
Number of training batches: 3483
Evaluation examples: 1743
Number of evaluation batches: 218
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.







































































































































































Loss: 4.0578: 100%|██████████| 3483/3483 [05:37<00:00, 10.32it/s]



Loss: 2.6997, F1: 0.3256, EM: 0.6390:  98%|█████████▊| 213/218 [00:06<00:00, 32.09it/s]
Results:
Train Loss: 4.0578
Val Loss: 2.6730
Precision: 0.3195
Recall: 0.3317
F1: 0.3255
Loss: 2.6730, F1: 0.3255, EM: 0.6431: 100%|██████████| 218/218 [00:07<00:00, 30.91it/s]







































































































































































Loss: 2.0524: 100%|██████████| 3483/3483 [05:37<00:00, 10.33it/s]



Loss: 2.7166, F1: 0.4023, EM: 0.6430:  98%|█████████▊| 213/218 [00:06<00:00, 32.27it/s]
Results:
Train Loss: 2.0524
Val Loss: 2.6904
Precision: 0.3600
Recall: 0.4554
F1: 0.4021
Loss: 2.6904, F1: 0.4021, EM: 0.6466: 100%|██████████| 218/218 [00:07<00:00, 31.07it/s]







































































































































































Loss: 1.2206: 100%|██████████| 3483/3483 [05:36<00:00, 10.34it/s]



Loss: 3.1752, F1: 0.4694, EM: 0.6667: 100%|██████████| 218/218 [00:06<00:00, 31.16it/s]
Results:
Train Loss: 1.2206
Val Loss: 3.1752
Precision: 0.4389
Recall: 0.5045
F1: 0.4694