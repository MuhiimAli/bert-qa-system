Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

































































































































































Loss: 3.6065: 100%|██████████| 1740/1740 [05:24<00:00,  5.36it/s]


Loss: 2.3107, F1: 0.4291, EM: 0.7022: 100%|██████████| 109/109 [00:07<00:00, 15.51it/s]
Loss: 1.3666:   0%|          | 2/1740 [00:00<06:43,  4.30it/s]
Results:
Train Loss: 3.6065
Val Loss: 2.3107
Precision: 0.3819
Recall: 0.4897
F1: 0.4291


































































































































































Loss: 1.7854: 100%|██████████| 1740/1740 [05:26<00:00,  5.33it/s]



Loss: 2.6075, F1: 0.4862, EM: 0.7077:  91%|█████████ | 99/109 [00:06<00:00, 15.71it/s]
Results:
Train Loss: 1.7854
Val Loss: 2.4241
Precision: 0.4418
Recall: 0.5217
F1: 0.4785
Loss: 2.4241, F1: 0.4785, EM: 0.7258: 100%|██████████| 109/109 [00:07<00:00, 15.41it/s]


































































































































































Loss: 1.0710: 100%|██████████| 1740/1740 [05:26<00:00,  5.33it/s]



Loss: 2.6786, F1: 0.5010, EM: 0.7167:  98%|█████████▊| 107/109 [00:06<00:00, 15.69it/s]
Results:
Train Loss: 1.0710
Val Loss: 2.6300
Precision: 0.4550
Recall: 0.5658

Loss: 2.6300, F1: 0.5044, EM: 0.7217: 100%|██████████| 109/109 [00:07<00:00, 15.46it/s]