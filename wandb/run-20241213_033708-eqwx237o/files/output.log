Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2973:   0%|          | 15/6966 [00:01<09:02, 12.81it/s]











































































































































































































































































Loss: 3.9660: 100%|██████████| 6966/6966 [08:57<00:00, 12.95it/s]




Loss: 2.7641, F1: 0.4388, EM: 0.6236: 100%|██████████| 436/436 [00:10<00:00, 41.75it/s]
Training:   0%|          | 0/6966 [00:00<?, ?it/s]
Results:
Train Loss: 3.9660
Val Loss: 2.7641
Precision: 0.4107
Recall: 0.4710
F1: 0.4388













































































































































































































































































Loss: 2.1534: 100%|██████████| 6966/6966 [08:59<00:00, 12.91it/s]




Loss: 3.3659, F1: 0.4749, EM: 0.6210:  85%|████████▌ | 371/436 [00:08<00:01, 42.52it/s]
Results:
Train Loss: 2.1534
Val Loss: 3.0566
Precision: 0.4240
Recall: 0.5301
F1: 0.4711
Loss: 3.0566, F1: 0.4711, EM: 0.6581: 100%|██████████| 436/436 [00:10<00:00, 41.70it/s]













































































































































































































































































Loss: 1.3798: 100%|██████████| 6966/6966 [09:00<00:00, 12.89it/s]




Loss: 4.1945, F1: 0.5148, EM: 0.6181:  82%|████████▏ | 356/436 [00:08<00:01, 42.54it/s]
Results:
Train Loss: 1.3798
Val Loss: 3.8868
Precision: 0.4795
Recall: 0.5324

Loss: 3.8868, F1: 0.5045, EM: 0.6517: 100%|██████████| 436/436 [00:10<00:00, 41.67it/s]