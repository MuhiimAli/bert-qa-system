Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2081:   0%|          | 2/870 [00:01<08:16,  1.75it/s]



























































































































































































































Loss: 4.8830: 100%|██████████| 870/870 [07:21<00:00,  1.97it/s]




Loss: 3.3241, F1: 0.3578, EM: 0.5525:  85%|████████▌ | 47/55 [00:08<00:01,  5.82it/s]
Results:
Train Loss: 4.8830
Val Loss: 3.0528
Precision: 0.3016
Recall: 0.4385
F1: 0.3574
Loss: 3.0528, F1: 0.3574, EM: 0.5915: 100%|██████████| 55/55 [00:09<00:00,  5.73it/s]





























































































































































































































Loss: 2.6657: 100%|██████████| 870/870 [07:23<00:00,  1.96it/s]



Loss: 3.1946, F1: 0.3603, EM: 0.5426:  80%|████████  | 44/55 [00:07<00:01,  5.81it/s]
Results:
Train Loss: 2.6657
Val Loss: 2.8587
Precision: 0.2792
Recall: 0.4610

Loss: 2.8587, F1: 0.3478, EM: 0.5932: 100%|██████████| 55/55 [00:09<00:00,  5.72it/s]