Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
























































































































































Loss: 4.0203: 100%|██████████| 870/870 [05:07<00:00,  2.83it/s]



Loss: 2.5911, F1: 0.4042, EM: 0.6900:  93%|█████████▎| 51/55 [00:06<00:00,  8.16it/s]
Results:
Train Loss: 4.0203
Val Loss: 2.4275
Precision: 0.3902
Recall: 0.4418
F1: 0.4144
Loss: 2.4275, F1: 0.4144, EM: 0.7068: 100%|██████████| 55/55 [00:06<00:00,  8.07it/s]

























































































































































Loss: 1.9562: 100%|██████████| 870/870 [05:08<00:00,  2.82it/s]


Loss: 2.9285, F1: 0.4132, EM: 0.6434:  71%|███████   | 39/55 [00:04<00:01,  8.19it/s]
Results:
Train Loss: 1.9562
Val Loss: 2.2288
Precision: 0.4125
Recall: 0.5035
F1: 0.4535
Loss: 2.2288, F1: 0.4535, EM: 0.7263: 100%|██████████| 55/55 [00:06<00:00,  8.07it/s]


























































































































































Loss: 1.3265: 100%|██████████| 870/870 [05:09<00:00,  2.81it/s]


Loss: 2.8994, F1: 0.4312, EM: 0.6600:  76%|███████▋  | 42/55 [00:05<00:01,  8.12it/s]
Results:
Train Loss: 1.3265
Val Loss: 2.3899
Precision: 0.3864
Recall: 0.5532

Loss: 2.3899, F1: 0.4550, EM: 0.7154: 100%|██████████| 55/55 [00:06<00:00,  8.05it/s]