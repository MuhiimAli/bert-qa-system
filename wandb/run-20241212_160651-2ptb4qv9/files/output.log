Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.1055:   0%|          | 15/6966 [00:01<09:05, 12.74it/s]













































































































































































































































































Loss: 3.8327: 100%|██████████| 6966/6966 [09:01<00:00, 12.86it/s]




Loss: 2.8081, F1: 0.4142, EM: 0.6231: 100%|██████████| 436/436 [00:10<00:00, 41.46it/s]
Training:   0%|          | 0/6966 [00:00<?, ?it/s]
Results:
Train Loss: 3.8327
Val Loss: 2.8081
Precision: 0.3731
Recall: 0.4656
F1: 0.4142














































































































































































































































































Loss: 2.0839: 100%|██████████| 6966/6966 [09:02<00:00, 12.84it/s]





Loss: 2.9953, F1: 0.4542, EM: 0.6428:  97%|█████████▋| 421/436 [00:10<00:00, 42.30it/s]
Results:
Train Loss: 2.0839
Val Loss: 2.9397
Precision: 0.4071
Recall: 0.5138

Loss: 2.9397, F1: 0.4543, EM: 0.6500: 100%|██████████| 436/436 [00:10<00:00, 41.44it/s]