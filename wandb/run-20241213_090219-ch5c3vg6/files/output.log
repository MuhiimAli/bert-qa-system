Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.2348:   0%|          | 15/6966 [00:01<08:58, 12.91it/s]












































































































































































































































































Loss: 3.8414: 100%|██████████| 6966/6966 [08:58<00:00, 12.93it/s]




Loss: 2.9381, F1: 0.4279, EM: 0.6093:  93%|█████████▎| 406/436 [00:09<00:00, 42.55it/s]
Results:
Train Loss: 3.8414
Val Loss: 2.8101
Precision: 0.3928
Recall: 0.4629
F1: 0.4250
Loss: 2.8101, F1: 0.4250, EM: 0.6271: 100%|██████████| 436/436 [00:10<00:00, 41.66it/s]












































































































































































































































































Loss: 2.0806: 100%|██████████| 6966/6966 [08:59<00:00, 12.91it/s]





Loss: 2.9959, F1: 0.4934, EM: 0.6450:  97%|█████████▋| 421/436 [00:10<00:00, 42.53it/s]
Results:
Train Loss: 2.0806
Val Loss: 2.9523
Precision: 0.4493
Recall: 0.5472

Loss: 2.9523, F1: 0.4934, EM: 0.6500: 100%|██████████| 436/436 [00:10<00:00, 41.61it/s]