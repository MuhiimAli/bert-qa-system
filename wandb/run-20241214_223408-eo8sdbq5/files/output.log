Training examples: 27866
Number of training batches: 3483
Evaluation examples: 1743
Number of evaluation batches: 218
Epoch 1/3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.






































































































































































Loss: 4.0583: 100%|██████████| 3483/3483 [05:35<00:00, 10.38it/s]



Loss: 2.8190, F1: 0.3905, EM: 0.6183:  94%|█████████▍| 205/218 [00:06<00:00, 32.00it/s]
Results:
Train Loss: 4.0583
Val Loss: 2.7272
Precision: 0.3480
Recall: 0.4173
F1: 0.3795
Loss: 2.7272, F1: 0.3795, EM: 0.6311: 100%|██████████| 218/218 [00:07<00:00, 30.85it/s]







































































































































































Loss: 2.0501: 100%|██████████| 3483/3483 [05:36<00:00, 10.35it/s]


Loss: 2.6902, F1: 0.4375, EM: 0.6569: 100%|██████████| 218/218 [00:07<00:00, 31.02it/s]
Training:   0%|          | 0/3483 [00:00<?, ?it/s]
Results:
Train Loss: 2.0501
Val Loss: 2.6902
Precision: 0.3947
Recall: 0.4908
F1: 0.4375








































































































































































Loss: 1.2200: 100%|██████████| 3483/3483 [05:37<00:00, 10.33it/s]



Loss: 3.1533, F1: 0.4639, EM: 0.6672: 100%|██████████| 218/218 [00:07<00:00, 30.87it/s]
Results:
Train Loss: 1.2200
Val Loss: 3.1533
Precision: 0.4280
Recall: 0.5064
F1: 0.4639