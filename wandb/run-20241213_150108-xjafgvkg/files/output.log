Training examples: 27847
Number of training batches: 870
Evaluation examples: 1743
Number of evaluation batches: 55
Epoch 1/2
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



















































































































































































































Loss: 3.6791: 100%|██████████| 870/870 [07:05<00:00,  2.05it/s]




Loss: 2.5186, F1: 0.3766, EM: 0.6691:  93%|█████████▎| 51/55 [00:08<00:00,  6.02it/s]
Results:
Train Loss: 3.6791
Val Loss: 2.3780
Precision: 0.3314
Recall: 0.4701
F1: 0.3887
Loss: 2.3780, F1: 0.3887, EM: 0.6862: 100%|██████████| 55/55 [00:09<00:00,  5.90it/s]




















































































































































































































Loss: 1.9169: 100%|██████████| 870/870 [07:07<00:00,  2.04it/s]




Loss: 2.4552, F1: 0.3950, EM: 0.6784:  87%|████████▋ | 48/55 [00:08<00:01,  6.02it/s]
Results:
Train Loss: 1.9169
Val Loss: 2.2377
Precision: 0.3305
Recall: 0.5340

Loss: 2.2377, F1: 0.4083, EM: 0.7022: 100%|██████████| 55/55 [00:09<00:00,  5.91it/s]