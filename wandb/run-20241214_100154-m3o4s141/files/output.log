Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Epoch 1/2
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_end.bias', 'distilbert.qa_end.weight', 'distilbert.qa_start.bias', 'distilbert.qa_start.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.





























































































































































Loss: 3.4874: 100%|██████████| 1740/1740 [05:17<00:00,  5.48it/s]



Loss: 2.5351, F1: 0.3690, EM: 0.6695:  89%|████████▉ | 97/109 [00:06<00:00, 15.98it/s]
Results:
Train Loss: 3.4874
Val Loss: 2.3554
Precision: 0.3019
Recall: 0.5142
F1: 0.3804
Loss: 2.3554, F1: 0.3804, EM: 0.6942: 100%|██████████| 109/109 [00:06<00:00, 15.72it/s]






























































































































































Loss: 1.7719: 100%|██████████| 1740/1740 [05:18<00:00,  5.46it/s]



Loss: 2.2919, F1: 0.3967, EM: 0.7172: 100%|██████████| 109/109 [00:06<00:00, 16.26it/s]
Results:
Train Loss: 1.7719
Val Loss: 2.2919
Precision: 0.3126
Recall: 0.5427

Loss: 2.2919, F1: 0.3967, EM: 0.7172: 100%|██████████| 109/109 [00:06<00:00, 15.66it/s]