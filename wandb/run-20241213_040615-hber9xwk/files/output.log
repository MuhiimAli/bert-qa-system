Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 12.8784:   0%|          | 15/6966 [00:01<08:59, 12.89it/s]











































































































































































































































































Loss: 3.7801: 100%|██████████| 6966/6966 [08:56<00:00, 12.98it/s]




Loss: 2.8299, F1: 0.4668, EM: 0.6091:  92%|█████████▏| 401/436 [00:09<00:00, 43.06it/s]
Results:
Train Loss: 3.7801
Val Loss: 2.7162
Precision: 0.4257
Recall: 0.5163
F1: 0.4667
Loss: 2.7162, F1: 0.4667, EM: 0.6277: 100%|██████████| 436/436 [00:10<00:00, 42.19it/s]










































































































































































































































































Loss: 2.0161: 100%|██████████| 6966/6966 [08:54<00:00, 13.03it/s]




Loss: 3.1656, F1: 0.4907, EM: 0.6300:  86%|████████▌ | 376/436 [00:08<00:01, 43.01it/s]
Results:
Train Loss: 2.0161
Val Loss: 3.0001
Precision: 0.4319
Recall: 0.5461

Loss: 3.0001, F1: 0.4823, EM: 0.6546: 100%|██████████| 436/436 [00:10<00:00, 42.18it/s]