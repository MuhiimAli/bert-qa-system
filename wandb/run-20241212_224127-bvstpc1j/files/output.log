Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 12.9844:   0%|          | 15/6966 [00:01<09:06, 12.73it/s]














































































































































































































































































Loss: 3.5354: 100%|██████████| 6966/6966 [09:02<00:00, 12.83it/s]




Loss: 2.8307, F1: 0.4382, EM: 0.6045:  89%|████████▊ | 386/436 [00:09<00:01, 42.33it/s]
Results:
Train Loss: 3.5354
Val Loss: 2.6543
Precision: 0.3907
Recall: 0.4953

Loss: 2.6543, F1: 0.4368, EM: 0.6311: 100%|██████████| 436/436 [00:10<00:00, 41.24it/s]