Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loss: 13.1199:   0%|          | 15/6966 [00:01<09:02, 12.81it/s]













































































































































































































































































Loss: 3.9377: 100%|██████████| 6966/6966 [09:01<00:00, 12.87it/s]




Loss: 2.9817, F1: 0.4279, EM: 0.5742:  85%|████████▌ | 371/436 [00:09<00:01, 42.26it/s]
Results:
Train Loss: 3.9377
Val Loss: 2.8348
Precision: 0.3311
Recall: 0.5415
F1: 0.4109
Loss: 2.8348, F1: 0.4109, EM: 0.6030: 100%|██████████| 436/436 [00:10<00:00, 41.49it/s]














































































































































































































































































Loss: 2.1912: 100%|██████████| 6966/6966 [09:02<00:00, 12.84it/s]





Loss: 2.8418, F1: 0.4798, EM: 0.6412:  99%|█████████▉| 431/436 [00:10<00:00, 42.32it/s]
Results:
Train Loss: 2.1912
Val Loss: 2.8261
Precision: 0.4451
Recall: 0.5205

Loss: 2.8261, F1: 0.4799, EM: 0.6420: 100%|██████████| 436/436 [00:10<00:00, 41.43it/s]