wandb: Currently logged in as: muhiim_ali (CS1460). Use `wandb login --relogin` to force relogin
wandb: Agent Starting Run: qlaqneh3 with config:
wandb: 	batch_size: 16
wandb: 	learning_rate: 3.428966291385484e-05
wandb: 	num_epochs: 2
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_122148-qlaqneh3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/qlaqneh3
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Create sweep with ID: nsggzshb
Sweep URL: https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109

Epoch 1/2
Training:   0%|          | 0/1740 [00:00<?, ?it/s]Training:   0%|          | 0/1740 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 267, in train
    train_loss = train_one_epoch(
  File "/oscar/home/mali37/bert-qa/train.py", line 206, in train_one_epoch
    outputs = model(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/oscar/home/mali37/bert-qa/train.py", line 32, in forward
    outputs = super().forward(
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 811, in forward
    return self.transformer(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 572, in forward
    layer_outputs = layer_module(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 498, in forward
    sa_output = self.attention(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 228, in forward
    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 51.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 658.00 MiB memory in use. Of the allocated memory 436.56 MiB is allocated by PyTorch, and 23.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.014 MB of 0.014 MB uploadedwandb:                                                                                
wandb: üöÄ View run fresh-sweep-1 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/qlaqneh3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_122148-qlaqneh3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run qlaqneh3 errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 267, in train
    train_loss = train_one_epoch(
  File "/oscar/home/mali37/bert-qa/train.py", line 206, in train_one_epoch
    outputs = model(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/oscar/home/mali37/bert-qa/train.py", line 32, in forward
    outputs = super().forward(
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 811, in forward
    return self.transformer(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 572, in forward
    layer_outputs = layer_module(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 498, in forward
    sa_output = self.attention(
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 228, in forward
    scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 51.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 658.00 MiB memory in use. Of the allocated memory 436.56 MiB is allocated by PyTorch, and 23.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qlaqneh3 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 267, in train
wandb: ERROR     train_loss = train_one_epoch(
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 206, in train_one_epoch
wandb: ERROR     outputs = model(
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 32, in forward
wandb: ERROR     outputs = super().forward(
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 811, in forward
wandb: ERROR     return self.transformer(
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 572, in forward
wandb: ERROR     layer_outputs = layer_module(
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 498, in forward
wandb: ERROR     sa_output = self.attention(
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 228, in forward
wandb: ERROR     scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 51.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 658.00 MiB memory in use. Of the allocated memory 436.56 MiB is allocated by PyTorch, and 23.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: csrakrk4 with config:
wandb: 	batch_size: 8
wandb: 	learning_rate: 3.0014502112805352e-05
wandb: 	num_epochs: 3
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_122406-csrakrk4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/csrakrk4
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.007 MB uploadedwandb: | 0.005 MB of 0.007 MB uploadedwandb: / 0.013 MB of 0.013 MB uploadedwandb:                                                                                
wandb: üöÄ View run generous-sweep-2 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/csrakrk4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_122406-csrakrk4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run csrakrk4 errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run csrakrk4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
wandb: ERROR     model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 2 more times]
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: rg6no1vr with config:
wandb: 	batch_size: 64
wandb: 	learning_rate: 3.7063542587845016e-05
wandb: 	num_epochs: 3
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_122618-rg6no1vr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/rg6no1vr
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.007 MB uploadedwandb: | 0.005 MB of 0.007 MB uploadedwandb: / 0.013 MB of 0.013 MB uploadedwandb:                                                                                
wandb: üöÄ View run olive-sweep-3 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/rg6no1vr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_122618-rg6no1vr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run rg6no1vr errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run rg6no1vr errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
wandb: ERROR     model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: naui8lyv with config:
wandb: 	batch_size: 64
wandb: 	learning_rate: 3.0094395066451897e-05
wandb: 	num_epochs: 2
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_122831-naui8lyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/naui8lyv
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.005 MB of 0.005 MB uploadedwandb: / 0.013 MB of 0.013 MB uploadedwandb:                                                                                
wandb: üöÄ View run ancient-sweep-4 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/naui8lyv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_122831-naui8lyv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run naui8lyv errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run naui8lyv errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
wandb: ERROR     model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: moaadmdw with config:
wandb: 	batch_size: 16
wandb: 	learning_rate: 3.564575941338049e-05
wandb: 	num_epochs: 3
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_123043-moaadmdw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lunar-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/moaadmdw
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.013 MB uploadedwandb: | 0.005 MB of 0.013 MB uploadedwandb: / 0.013 MB of 0.013 MB uploadedwandb:                                                                                
wandb: üöÄ View run lunar-sweep-5 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/moaadmdw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_123043-moaadmdw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run moaadmdw errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run moaadmdw errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
wandb: ERROR     model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: jwsl5l9k with config:
wandb: 	batch_size: 16
wandb: 	learning_rate: 3.6260685879600574e-05
wandb: 	num_epochs: 1
wandb: wandb version 0.19.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /oscar/home/mali37/bert-qa/wandb/run-20241213_123256-jwsl5l9k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/CS1460/nq-qa-bert3
wandb: üßπ View sweep at https://wandb.ai/CS1460/nq-qa-bert3/sweeps/nsggzshb
wandb: üöÄ View run at https://wandb.ai/CS1460/nq-qa-bert3/runs/jwsl5l9k
Some weights of DistilBertForQA were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.qa_outputs.bias', 'distilbert.qa_outputs.weight', 'distilbert.qa_type.bias', 'distilbert.qa_type.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.005 MB of 0.005 MB uploadedwandb: \ 0.005 MB of 0.005 MB uploadedwandb: | 0.013 MB of 0.013 MB uploadedwandb:                                                                                
wandb: üöÄ View run laced-sweep-6 at: https://wandb.ai/CS1460/nq-qa-bert3/runs/jwsl5l9k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/CS1460/nq-qa-bert3
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241213_123256-jwsl5l9k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Run jwsl5l9k errored:
Traceback (most recent call last):
  File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
    self._function()
  File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
    return train_with_params(args, use_wandb=True)
  File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
    model = train(args, data, tokenizer, use_wandb)
  File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
    model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
  File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
    return self._apply(convert)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
    module._apply(fn)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
    param_applied = fn(param)
  File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
    return t.to(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jwsl5l9k errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/wandb/agents/pyagent.py", line 307, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 72, in run_sweep
wandb: ERROR     return train_with_params(args, use_wandb=True)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/main.py", line 48, in train_with_params
wandb: ERROR     model = train(args, data, tokenizer, use_wandb)
wandb: ERROR   File "/oscar/home/mali37/bert-qa/train.py", line 249, in train
wandb: ERROR     model = DistilBertForQA.from_pretrained('distilbert-base-uncased').to(device)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/transformers/modeling_utils.py", line 2724, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1340, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 900, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR   File "/users/mali37/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1326, in convert
wandb: ERROR     return t.to(
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 9.06 MiB is free. Process 2460814 has 22.93 GiB memory in use. Including non-PyTorch memory, this process has 700.00 MiB memory in use. Of the allocated memory 485.08 MiB is allocated by PyTorch, and 16.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
Detected 5 failed runs in a row at start, killing sweep.
wandb: ERROR Detected 5 failed runs in a row at start, killing sweep.
wandb: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val
Training examples: 27847
Number of training batches: 3480
Evaluation examples: 1743
Number of evaluation batches: 218
Training examples: 27847
Number of training batches: 435
Evaluation examples: 1743
Number of evaluation batches: 28
Training examples: 27847
Number of training batches: 435
Evaluation examples: 1743
Number of evaluation batches: 28
Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
Training examples: 27847
Number of training batches: 1740
Evaluation examples: 1743
Number of evaluation batches: 109
